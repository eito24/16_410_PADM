{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 04: HMMs\n",
    "\n",
    "In this problem set you will implement value iteration in a non-deterministic MDP, and HMM Filtering.\n",
    "\n",
    "\n",
    "0. [Credit for Contributors (required)](#contributors)\n",
    "\n",
    "      \n",
    "1. [HMM Filtering](#problem1)\n",
    "    1. [Implement `compute_casino_table` for fair dice estimation (40 points)](#compute_casino_table)\n",
    "    2. [Dendrochronology Example (20 points)](#viterbi)\n",
    "    \n",
    "**60 points** total for Problem Set 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"contributors\"></a> Credit for Contributors\n",
    "\n",
    "List the various students, lecture notes, or online resouces that helped you complete this problem set:\n",
    "\n",
    "Ex: I worked with Bob on the cat activity planning problem.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Write your answer in the cell below this one.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden Markov Models Lecture Notes\n",
    "AIMA Ch 14.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful imports\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from nose.tools import ok_, assert_equal, assert_almost_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"problem1\"/></a> Problem 1 : HMM  Filtering (60 points)\n",
    "\n",
    "In this problem you will perform filtering on an HMM using the forward algorithm (as explained in class).\n",
    "\n",
    "Remember that the filtering problem answers the question\n",
    "\n",
    "\\begin{equation}\n",
    "Pr(S_t | o_{1:t})\n",
    "\\end{equation}\n",
    "\n",
    "that is, what is the most likely state at time $t$ after reading $t$ observations.\n",
    "\n",
    "You will implement filtering and demonstrate it for the \"dishonest casino\" example HMM that was introduced in class. The parameters for the HMM are shown in the following image (and also in the lecture notes).\n",
    "\n",
    "<img src='hmm_casino.png'/>\n",
    "\n",
    "Also, the prior for the states is\n",
    "\n",
    "\\begin{align}\n",
    "    &Pr(S_0 = Fair) &= 0.8\\\\\n",
    "    &Pr(S_0 = Loaded) &= 0.2\n",
    "\\end{align}\n",
    "\n",
    "that is, the probability that the initial state is \"Fair\" is 0.8, and the probability that the initial state is \"Loaded\" is 0.2.\n",
    "\n",
    "Implement filtering for this HMM for the following observation sequence:\n",
    "\n",
    "`['1','2','4','6','6','6','3','6']`\n",
    "\n",
    "Your code should produce a table like the following one (from 0 measurements to the totality of the measurements provided).\n",
    "\n",
    "\n",
    "```\n",
    "Filtering for the dishonest casino HMM:\n",
    "  with 0 measurement(s): [], P(s at t=0) is: [Fair: 0.8000,  Loaded: 0.2000]\n",
    "  with 1 measurement(s): ['1'], P(s at t=1) is: [Fair: 0.8480,  Loaded: 0.1520]\n",
    "  with 2 measurement(s): ['1', '2'], P(s at t=2) is: [Fair: 0.8789,  Loaded: 0.1211]\n",
    "  with 3 measurement(s): ['1', '2', '4'], P(s at t=3) is: [Fair: 0.8981,  Loaded: 0.1019]\n",
    "  with 4 measurement(s): ['1', '2', '4', '6'], P(s at t=4) is: [Fair: 0.6688,  Loaded: 0.3312]\n",
    "  ...\n",
    "```\n",
    "\n",
    "Note that this is exactly the example given in class and that you have the results in the slides.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Implement HMM filtering and use it in the \"dishonest casino\" example HMM to generate a table like the one above.\n",
    "</div>\n",
    "\n",
    "\n",
    "**Note**: you could implement `smoothing` and `decoding` on the same HMM model and observation data for **additional credit** (but you don't have to if you don't want). \n",
    "\n",
    "If you do implement smoothing and decoding, please make it explicit in a separate cell below for our graders to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix M is:\n",
      "[[3.5 2.1 1.3]\n",
      " [0.1 0.2 0.3]]\n",
      "Element (0,1) of M is:  2.1\n",
      "Shape (dimension) of M is:  (2, 3)\n",
      "Vector v is:\n",
      "[1 3]\n",
      "2\n",
      "Matrix multiplication v * M =  [3.8 2.7 2.2]\n",
      "Element wise v * a = [1 3] * [2 4] =  [ 2 12]\n",
      "Sum of elements of v ([1 3]) is: 4\n",
      "Vector b is:  [ 2  4  6  8 10 12 14 16]\n",
      "First 3 elements of b are:  [2 4 6]\n",
      "Elements from the second until the end are:  [ 4  6  8 10 12 14 16]\n",
      "Elements from 4th to 7th are:  [ 8 10 12 14]\n"
     ]
    }
   ],
   "source": [
    "# You might find the following examples on how to use numpy for matrices and vectors useful\n",
    "\n",
    "# Define a 2x3 Matrix\n",
    "M=np.array([[3.5, 2.1, 1.3],\n",
    "            [0.1, 0.2, 0.3]])\n",
    "print(\"Matrix M is:\")\n",
    "print(M)\n",
    "\n",
    "print(\"Element (0,1) of M is: \", M[0,1])\n",
    "print(\"Shape (dimension) of M is: \", M.shape)\n",
    "\n",
    "# Define 1x2 vector\n",
    "v =  np.array([1, 3])\n",
    "print(\"Vector v is:\")\n",
    "print(v)\n",
    "\n",
    "# Matrix multiplication r = v*M\n",
    "r = v.dot(M)\n",
    "print(\"Matrix multiplication v * M = \", r)\n",
    "\n",
    "# Element wise multiplication\n",
    "a = np.array([2, 4])\n",
    "print(\"Element wise v * a = %s * %s = \"%(v,a), v*a)\n",
    "\n",
    "# Sum of elements of v\n",
    "print(\"Sum of elements of v (%s) is: %s\" %(v, np.sum(v)))\n",
    "\n",
    "# Vector slicing\n",
    "b = np.array([2,4,6,8,10,12,14,16])\n",
    "print(\"Vector b is: \", b)\n",
    "print(\"First 3 elements of b are: \", b[:3])\n",
    "print(\"Elements from the second until the end are: \", b[1:])\n",
    "print(\"Elements from 4th to 7th are: \", b[3:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###  <a name=\"compute_casino_table\"></a>Implement HMM Filter\n",
    "\n",
    "Write your code as a function:\n",
    "\n",
    "```\n",
    "def compute_casino_table(meas)\n",
    "```\n",
    "\n",
    "that takes as input the observation sequence, and prints the table discussed above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4646116d050a53ccd35b19a81f10537f",
     "grade": true,
     "grade_id": "hmm_filtering_code",
     "locked": false,
     "points": 40,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_casino_table(meas):\n",
    "    # Example observation sequence\n",
    "    # meas = ['1','2','4','6','6','6','3','6']\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    t = int(len(meas))\n",
    "    table = np.zeros([t+1,2])\n",
    "    for i in range(t+1):\n",
    "        if i == 0:\n",
    "            table[i,0] = 0.8 #Pr(S0=Fair)=0.8\n",
    "            table[i,1] = 0.2 #Pr(S0=Fair)=0.2\n",
    "        else:\n",
    "            rolled = meas[i-1]\n",
    "            if rolled == '6':\n",
    "                p_fair = (1/6)*((0.95*table[i-1,0])+(0.05*table[i-1,1]))\n",
    "                p_loaded = (1/2)*((0.05*table[i-1,0])+(0.95*table[i-1,1]))\n",
    "                alpha = 1/(p_fair+p_loaded)\n",
    "                table[i,0] = alpha*p_fair\n",
    "                table[i,1] = alpha*p_loaded\n",
    "            else:\n",
    "                p_fair = (1/6)*((0.95*table[i-1,0])+(0.05*table[i-1,1]))\n",
    "                p_loaded = (1/10)*((0.05*table[i-1,0])+(0.95*table[i-1,1]))\n",
    "                alpha = 1/(p_fair+p_loaded)\n",
    "                table[i,0] = alpha*p_fair\n",
    "                table[i,1] = alpha*p_loaded\n",
    "    print(\"Filtering for the dishonest casino HMM:\")\n",
    "    for i in range(t+1):\n",
    "        print(f\"\\t with {i} measurement(s): {meas[:i]}, P(s at t={i}) is: [Fair: {round(table[i,0],4)}, Loaded: {round(table[i,1],4)}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8662203bd3b0578a4ec619b2fd99c78",
     "grade": false,
     "grade_id": "check_hmm",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for the dishonest casino HMM:\n",
      "\t with 0 measurement(s): [], P(s at t=0) is: [Fair: 0.8, Loaded: 0.2]\n",
      "\t with 1 measurement(s): ['1'], P(s at t=1) is: [Fair: 0.848, Loaded: 0.152]\n",
      "\t with 2 measurement(s): ['1', '2'], P(s at t=2) is: [Fair: 0.8789, Loaded: 0.1211]\n",
      "\t with 3 measurement(s): ['1', '2', '4'], P(s at t=3) is: [Fair: 0.8981, Loaded: 0.1019]\n",
      "\t with 4 measurement(s): ['1', '2', '4', '6'], P(s at t=4) is: [Fair: 0.6688, Loaded: 0.3312]\n",
      "\t with 5 measurement(s): ['1', '2', '4', '6', '6'], P(s at t=5) is: [Fair: 0.3843, Loaded: 0.6157]\n",
      "\t with 6 measurement(s): ['1', '2', '4', '6', '6', '6'], P(s at t=6) is: [Fair: 0.1793, Loaded: 0.8207]\n",
      "\t with 7 measurement(s): ['1', '2', '4', '6', '6', '6', '3'], P(s at t=7) is: [Fair: 0.3088, Loaded: 0.6912]\n",
      "\t with 8 measurement(s): ['1', '2', '4', '6', '6', '6', '3', '6'], P(s at t=8) is: [Fair: 0.1399, Loaded: 0.8601]\n"
     ]
    }
   ],
   "source": [
    "# Don't modify this cell\n",
    "meas = ['1','2','4','6','6','6','3','6']\n",
    "compute_casino_table(meas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###  <a name=\"viterbi\"></a>Dendrochronology Example\n",
    " \n",
    "Dendrochronology (or tree-ring dating) is the scientific method of dating tree rings (also called growth rings) to the exact year they were formed in order to analyze atmospheric conditions during different periods in history. [Wikipedia]\n",
    "\n",
    "<img src=\"tree-rings-diagram.jpg\" style=\"width:40%;\"/>\n",
    "<p style=\"text-align:center\">Image source: NASA </p>\n",
    "\n",
    "Let’s say we are interested in estimating the average annual temperature at a particular location\n",
    "on Earth. However, the time period that we are interested in is in distant past, with no direct\n",
    "temperature data available. Prior research indicates correlation between size of tree rings and temperature, so we decide to use this as our evidence.\n",
    "\n",
    "To simplify the problem, we’ll only consider two temperature states, “Hot” and “Cold”. Suppose\n",
    "that we are told the probability of hot year followed by another hot year is 0.7 and the probability\n",
    "that a cold year is followed by another cold year is 0.6. Assume that the initial state distribution, π, is given. For observations, we’ll only consider three different tree ring sizes, small, medium, and large, or S, M , and L, respectively. The probabilistic relationship between the annual temperature and the ring sizes is the observation matrix. The state transition matrix, initial distribution, and observation matrix are provided below:\n",
    "\n",
    "<img src=\"temp_example.png\"/>\n",
    "\n",
    "For example, the probability of observing a “Large” tree ring size given that the state is “Hot” is 0.5.\n",
    "\n",
    "\n",
    "From a particular four-year period of interest, we observe the series of tree rings, **[S, M, S, L]**. \n",
    "Assume that $\\pi$ is for $s_0$ and the observations are for $s_1, \\ldots, s_4$ i.e $o_1 = $ **S** and so on. \n",
    "\n",
    "With this observation sequence, find the most likely temperature sequence $[s_1, s_2, s_3, s_4]$ using the **Viterbi\n",
    "algorithm**. \n",
    "\n",
    "Please show how you found the most likely sequence.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Write or upload your answer in the cell below this one.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f257d7f1cc4545580cd8a0c3c5b8e9d1",
     "grade": true,
     "grade_id": "hmm_work",
     "locked": false,
     "points": 20,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'H', 'C', 'H']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C', 'H', 'C', 'H']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_all = dict()\n",
    "dict_all[\"H\"] = {\"H\":0.7,\"C\":0.3,\"S\":0.1,\"M\":0.4,\"L\":0.5}\n",
    "dict_all[\"C\"] = {\"H\":0.4,\"C\":0.6,\"S\":0.7,\"M\":0.2,\"L\":0.1}\n",
    "def viterbi(observations):\n",
    "    t = int(len(observations))\n",
    "    table = np.zeros([t+1,2])\n",
    "    for i in range(t+1):\n",
    "        if i == 0:\n",
    "            table[i,0] = 0.6\n",
    "            table[i,1] = 0.4\n",
    "        else:\n",
    "            observation = observations[i-1] # = X_t+1\n",
    "            prev_hot = table[i-1,0]\n",
    "            prev_cold = table[i-1,1]\n",
    "            # for hot\n",
    "            H_to_H = prev_hot*dict_all[\"H\"][\"H\"]\n",
    "            C_to_H = prev_cold*dict_all[\"C\"][\"H\"]\n",
    "            p_evid_hot = dict_all[\"H\"][observation]\n",
    "            table[i,0] = max(H_to_H,C_to_H)*p_evid_hot\n",
    "            # for cold\n",
    "            H_to_C = prev_hot*dict_all[\"H\"][\"C\"]\n",
    "            C_to_C = prev_cold*dict_all[\"C\"][\"C\"]\n",
    "            p_evid_cold = dict_all[\"C\"][observation]\n",
    "            table[i,1] = max(H_to_C,C_to_C)*p_evid_cold\n",
    "    most_possible = []\n",
    "    for i in range(1,t+1):\n",
    "        if table[i,0]>table[i,1]:\n",
    "            most_possible.append(\"H\")\n",
    "        else:\n",
    "            most_possible.append(\"C\")\n",
    "    print(most_possible)\n",
    "    return most_possible\n",
    "observations = [\"S\",\"M\",\"S\",\"L\"]\n",
    "viterbi(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
